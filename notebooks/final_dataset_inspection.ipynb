{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from local file /Users/ludwig/research/deep_learning/imagenet_2/data/cache/metadata/imagenet_metadata_2018-09-14_01-26-58_UTC.pickle ... done\n",
      "Loaded 200418 unique candidates from 190 search result JSON file(s).\n",
      "    /Users/ludwig/research/deep_learning/imagenet_2/data/search_results/...\n",
      "        2018-07-31_flickr_search_result_vaishaal_class_1_153.json\n",
      "        2018-08-20-16-10-18_becca.json\n",
      "        2018-08-25-11-43-09_becca.json\n",
      "        2018-08-27-22-53-45_becca.json\n",
      "        2018-08-30-02-40-26_becca.json\n",
      "        2018-08-30-18-46-35_becca.json\n",
      "        2018-08-30-19-31-10_becca.json\n",
      "        2018-09-04-17-03-01_becca.json\n",
      "        2018-09-04-17-36-03_becca.json\n",
      "        2018-09-05-16-16-14_becca.json\n",
      "        ...\n",
      "    There were 81754 duplicate occurences.\n",
      "    Ignored 0 candidate entries because they are on the blacklist (blacklist size: 24363).\n",
      "Loaded 7040 HITs from 64 hit data JSON file(s) in 0 seconds.\n",
      "    /Users/ludwig/research/deep_learning/imagenet_2/data/mturk/hit_data_live/...\n",
      "        becca_hits_submitted_2018-09-20-21:12:45-PDT.json\n",
      "        becca_hits_submitted_2018-09-21-17:45:49-UTC.json\n",
      "        becca_hits_submitted_2018-09-21-22:55:10-UTC.json\n",
      "        ...\n",
      "    Ignored 7 filenames.\n",
      "    Ignored 4 HITs from the blacklist (blacklist size 4)\n",
      "Reading from local file /Users/ludwig/research/deep_learning/imagenet_2/data/cache/mturk_results/data_live_2018-11-16_21-04-47_UTC.pickle ... done\n",
      "Using pickled JSON data stored by ubuntu from /home/ubuntu/imagenet_2/data/mturk/hit_data_live locally\n",
      "    S3 source: s3://imagenet2datav2/mturk_results/data_live_2018-11-16_21-04-47_UTC.pickle\n",
      "    Ignored assignment data for 32 HITs\n",
      "    0 HITs do not have assignment data\n",
      "Loaded review data from /Users/ludwig/research/deep_learning/imagenet_2/data/metadata/nearest_neighbor_reviews_v2.json\n",
      "    Review info data 4836 candidates\n",
      "Loaded near duplicate resolution override data from /Users/ludwig/research/deep_learning/imagenet_2/data/metadata/near_duplicate_resolution_override.json\n",
      "    1 resolution overrides\n",
      "Reading from local file /Users/ludwig/research/deep_learning/imagenet_2/data/cache/review_thresholds/data_2018-11-17_00-24-27_UTC.pickle ... done\n",
      "Loaded review thresholds from s3://imagenet2datav2/review_thresholds/data_2018-11-17_00-24-27_UTC.pickle\n",
      "Loaded near duplicate data from /Users/ludwig/research/deep_learning/imagenet_2/data/metadata/near_duplicates.json\n",
      "    Near duplicate data for 6223 candidates\n",
      "Computing connected components ... done, took 0.1019954719999987 seconds\n",
      "There are 13472 non-singleton components\n",
      "Processing components ... done, took 0.9127377519999982 seconds\n",
      "100 non-singleton components contain at least one ImageNet image\n",
      "Currently 26668 candidates are marked as near-duplicates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ceb42ab76c64c82939a9d94e40330bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<style>\\n.image_grid_caption {\\n    font-size: 8px;\\n    line-height: 12px;\\n    height: 12px;\\n…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "repo_root = os.path.join(os.getcwd(), '../code')\n",
    "sys.path.append(repo_root)\n",
    "\n",
    "import candidate_data\n",
    "import dataset_sampling\n",
    "import final_dataset_inspection_notebook_code as notebook_code\n",
    "import image_loader\n",
    "import imagenet\n",
    "import mturk_data\n",
    "import mturk_utils\n",
    "import near_duplicate_data\n",
    "import utils\n",
    "\n",
    "imgnet = imagenet.ImageNetData()\n",
    "cds = candidate_data.CandidateData(load_metadata_from_s3=False, exclude_blacklisted_candidates=False)\n",
    "loader = image_loader.ImageLoader(imgnet, cds)\n",
    "mturk = mturk_data.MTurkData(live=True,\n",
    "                             load_assignments=True,\n",
    "                             source_filenames_to_ignore=mturk_data.main_collection_filenames_to_ignore)\n",
    "ndc = near_duplicate_data.NearDuplicateData(imgnet=imgnet,\n",
    "                                            candidates=cds,\n",
    "                                            mturk_data=mturk,\n",
    "                                            load_review_thresholds=True)\n",
    "\n",
    "display(mturk_utils.get_dataset_inspection_css())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagenetv2-c-5.json\n",
      "\n",
      "Generated by ludwig at 2018-11-26_02:30:16_UTC\n",
      "Sampling function: sample_best (seed 285740942)\n",
      "\n",
      "Futher parameters:\n",
      "    min_num_annotations: 10\n",
      "    near_duplicate_review_targets: {'l2': 120000000.0, 'dssim': 0.2205, 'fc7': 13200.0}\n",
      "    is_valid: False\n",
      "    starting_from: imagenetv2-c-4.json\n",
      "\n",
      "9996 images for 1000 wnids\n",
      "\n",
      "998 wnids have 10 images\n",
      "2 wnids have 8 images\n",
      "\n",
      "Number of unique image filenames: 9996\n",
      "\n",
      "9996 images are candidate images\n",
      "0 images are ImageNet images\n",
      "    0 training images\n",
      "    0 val images\n",
      "    0 test images\n",
      "\n",
      "Dataset wnids match the ImageNet wnids\n",
      "\n",
      "The dataset contains 0 blacklisted candidates\n",
      "\n",
      "The minimum number of assignments for an image in the dataset is 10\n",
      "\n",
      "The minimum image selection frequency of an image in the dataset is 0.5\n",
      "    (0.5 among wnids without a special threshold)\n",
      "    (0 images do not have a selection frequency)\n",
      "The average image selection frequency of the images in the dataset is 0.92\n",
      "\n",
      "The dataset contains 0 near-duplicates\n",
      "Review thresholds:\n",
      "    l2: 1.200295e+08\n",
      "    fc7: 1.321616e+04\n",
      "    dssim: 2.205036e-01\n",
      "\n",
      "Loading review data from /Users/ludwig/research/deep_learning/imagenet_2/data/dataset_reviews/imagenetv2-c-5_review.json\n",
      "\n",
      "Number of reviewed wnids: 981\n",
      "Number of problematic wnids: 0\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'imagenetv2-c-5.json'\n",
    "\n",
    "data, review_data, review_filepath = notebook_code.load_dataset_and_print_info(dataset_name, imgnet, cds, mturk, ndc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image data ... done, took 4.059256853000022 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8aad94223943d188399a0a004241a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='n01669191', _dom_classes=('wnid_heading',)), Label(value='box turtle, box tortoise…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_wnids_to_show = 15\n",
    "starting_wnid = 'n01669191'\n",
    "#starting_wnid = sorted(imgnet.class_info_by_wnid.keys())[0]\n",
    "\n",
    "image_filter = 'all'\n",
    "show_reviewed = False\n",
    "\n",
    "ui, review_checkboxes, problematic_checkboxes, blacklist_checkboxes, near_duplicate_text_fields = notebook_code.generate_review_ui(\n",
    "        num_wnids_to_show=num_wnids_to_show,\n",
    "        starting_wnid=starting_wnid,\n",
    "        data=data,\n",
    "        review_data=review_data,\n",
    "        imgnet=imgnet,\n",
    "        cds=cds,\n",
    "        loader=loader,\n",
    "        ndc=ndc,\n",
    "        num_val_images_per_wnid=10,\n",
    "        image_filter=image_filter,\n",
    "        show_reviewed=show_reviewed)\n",
    "\n",
    "ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 candidates to the blacklist (removed 0) for wnid n03697007 (lumbermill, sawmill)\n",
      "Added 1 candidates to the blacklist (removed 0) for wnid n04328186 (stopwatch, stop watch)\n",
      "\n",
      "\n",
      "Wrote updated review data to /Users/ludwig/research/deep_learning/imagenet_2/data/dataset_reviews/imagenetv2-c-4_review.json\n",
      "\n",
      "Number of reviewed wnids: 1000\n",
      "Number of problematic wnids: 19\n"
     ]
    }
   ],
   "source": [
    "with open('../data/metadata/candidate_blacklist.json', 'r') as f:\n",
    "    blacklist = json.load(f)\n",
    "num_added_by_wnid = {}\n",
    "num_removed_by_wnid = {}\n",
    "for cid, checkbox in blacklist_checkboxes.items():\n",
    "    assert cid in cds.all_candidates\n",
    "    cur_wnid = cds.all_candidates[cid]['wnid']\n",
    "    if checkbox.value:\n",
    "        if cid not in blacklist:\n",
    "            blacklist[cid] = 'invalid image (selected in the final review notebook)'\n",
    "            if cur_wnid not in num_added_by_wnid:\n",
    "                num_added_by_wnid[cur_wnid] = 0\n",
    "            num_added_by_wnid[cur_wnid] += 1\n",
    "relevant_wnids = sorted(list(set(num_added_by_wnid.keys()) | set(num_removed_by_wnid.keys())))\n",
    "for cur_wnid in relevant_wnids:\n",
    "    num_added = num_added_by_wnid[cur_wnid] if cur_wnid in num_added_by_wnid else 0\n",
    "    num_removed = num_removed_by_wnid[cur_wnid] if cur_wnid in num_removed_by_wnid else 0\n",
    "    cur_synset = ', '.join(imgnet.class_info_by_wnid[cur_wnid].synset)\n",
    "    print(f'Added {num_added} candidates to the blacklist (removed {num_removed}) for wnid {cur_wnid} ({cur_synset})')\n",
    "\n",
    "with open('../data/metadata/candidate_blacklist.json', 'w') as f:\n",
    "    json.dump(blacklist, f, indent=2, sort_keys=True)\n",
    "\n",
    "    \n",
    "print()\n",
    "\n",
    "\n",
    "with open('../data/metadata/near_duplicates.json', 'r') as f:\n",
    "    near_duplicates = json.load(f)\n",
    "for wnid, text_field in near_duplicate_text_fields.items():\n",
    "    cids = text_field.value.split(' ')\n",
    "    cids = [x.strip() for x in cids]\n",
    "    if len(cids) == 1 and cids[0] == '':\n",
    "        continue\n",
    "    assert len(cids) >= 2\n",
    "    if len(cids) != len(set(cids)):\n",
    "        print(f'Near-duplicate set {cids} has repeated elements')\n",
    "    assert len(cids) == len(set(cids))\n",
    "    for cid in cids:\n",
    "        assert cid in cds.all_candidates\n",
    "    root_cid = cids[0]\n",
    "    if root_cid not in near_duplicates:\n",
    "        near_duplicates[root_cid] = []\n",
    "    for other_cid in cids[1:]:\n",
    "        if other_cid not in near_duplicates[root_cid]:\n",
    "            near_duplicates[root_cid].append(other_cid)\n",
    "    near_duplicates[root_cid] = list(sorted(set(near_duplicates[root_cid])))\n",
    "    print('Added {} near-duplicates for wnid {} ({})'.format(len(cids), wnid, ', '.join(imgnet.class_info_by_wnid[wnid].synset)))\n",
    "        \n",
    "with open('../data/metadata/near_duplicates.json', 'w') as f:\n",
    "    json.dump(near_duplicates, f, indent=2, sort_keys=True)\n",
    "    \n",
    "print()\n",
    "\n",
    "review_filename = dataset_name[:-5] + '_review.json'\n",
    "review_filepath = pathlib.Path('../data/dataset_reviews/' + review_filename)\n",
    "review_filepath = review_filepath.resolve()\n",
    "assert review_filepath.is_file()\n",
    "with open(review_filepath, 'r') as f:\n",
    "    review_data = json.load(f)\n",
    "\n",
    "for wnid, checkbox in review_checkboxes.items():\n",
    "    review_data[wnid]['reviewed'] = checkbox.value\n",
    "\n",
    "for wnid, checkbox in problematic_checkboxes.items():\n",
    "    review_data[wnid]['problematic'] = checkbox.value\n",
    "\n",
    "with open(review_filepath, 'w') as f:\n",
    "    json.dump(review_data, f, indent=2, sort_keys=True)\n",
    "\n",
    "print('Wrote updated review data to {}'.format(review_filepath))\n",
    "print()\n",
    "num_reviewed = len([x for x in review_data.items() if x[1]['reviewed']])\n",
    "print('Number of reviewed wnids: {}'.format(num_reviewed))\n",
    "num_problematic = len([x for x in review_data.items() if x[1]['problematic']])\n",
    "print('Number of problematic wnids: {}'.format(num_problematic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n01669191', 1), ('n02066245', 28)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(notebook_code.compute_splitting_points(review_data, [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.66666666666667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "263 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(260 - 76) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
